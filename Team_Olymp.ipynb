{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü©∫ Explainable Vision-Language Model for Radiology (LLaVA-style, Colab-ready)\n",
        "\n",
        "**End-to-end, runnable Google Colab notebook**  \n",
        "GPU: **NVIDIA T4**  \n",
        "Frameworks: **PyTorch + Hugging Face Transformers**  \n",
        "Dataset: **ROCOv2-radiology (Hugging Face)**\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Environment Setup\n"
      ],
      "metadata": {
        "id": "6Ba7_mq4lsyV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DDzEwe3RBwG"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q transformers accelerate datasets sentencepiece\n",
        "!pip install -q scikit-learn pillow matplotlib opencv-python\n",
        "!pip install -q pytorch-grad-cam\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Dataset Loading"
      ],
      "metadata": {
        "id": "YxfimilLmLYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"roco\", \"radiology\")\n",
        "\n",
        "print(dataset)\n",
        "print(dataset[\"train\"][0].keys())"
      ],
      "metadata": {
        "id": "QxopwkRWRHYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Model Initialization"
      ],
      "metadata": {
        "id": "gMj0C4Kkmb3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"Vision tower:\", model.vision_tower.__class__)"
      ],
      "metadata": {
        "id": "Qqgmumy2RNfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Data Preprocessing & Prompting"
      ],
      "metadata": {
        "id": "-56RUcL9mt35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "MEDICAL_CONCEPTS = [\n",
        "    \"pneumonia\",\n",
        "    \"edema\",\n",
        "    \"fracture\",\n",
        "    \"tumor\",\n",
        "    \"hemorrhage\",\n",
        "    \"infection\",\n",
        "    \"lesion\",\n",
        "    \"effusion\",\n",
        "    \"nodule\",\n",
        "    \"cardiomegaly\"\n",
        "]\n",
        "\n",
        "NUM_LABELS = len(MEDICAL_CONCEPTS)\n",
        "\n",
        "def extract_concepts(caption):\n",
        "    \"\"\"Weak multi-label extraction via keyword matching\"\"\"\n",
        "    caption = caption.lower()\n",
        "    labels = [1 if concept in caption else 0 for concept in MEDICAL_CONCEPTS]\n",
        "    return torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "def build_prompt():\n",
        "    return (\n",
        "        \"You are an expert radiologist. \"\n",
        "        \"Given the medical image, generate a concise and accurate radiology report.\"\n",
        "    )\n",
        "\n",
        "def preprocess(example):\n",
        "    image = example[\"image\"]\n",
        "    caption = example[\"caption\"]\n",
        "\n",
        "    prompt = build_prompt()\n",
        "    labels = extract_concepts(caption)\n",
        "\n",
        "    inputs = processor(\n",
        "        images=image,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "    inputs[\"labels_text\"] = caption\n",
        "    inputs[\"labels_concepts\"] = labels\n",
        "    return inputs\n",
        "\n",
        "train_ds = dataset[\"train\"].shuffle(seed=seed).select(range(2000)).map(preprocess)\n",
        "val_ds   = dataset[\"validation\"].shuffle(seed=seed).select(range(500)).map(preprocess)\n",
        "test_ds  = dataset[\"test\"].shuffle(seed=seed).select(range(500)).map(preprocess)\n"
      ],
      "metadata": {
        "id": "zgb0zI30RUpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Training Loop (Report + Concept Loss)"
      ],
      "metadata": {
        "id": "zkat6zIImyRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from transformers import get_scheduler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ConceptClassifier(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_labels):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "vision_hidden = model.vision_tower.config.hidden_size\n",
        "classifier = ConceptClassifier(vision_hidden, NUM_LABELS).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    list(model.parameters()) + list(classifier.parameters()),\n",
        "    lr=2e-5\n",
        ")\n",
        "\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def collate_fn(batch):\n",
        "    keys = batch[0].keys()\n",
        "    out = {}\n",
        "    for k in keys:\n",
        "        if torch.is_tensor(batch[0][k]):\n",
        "            out[k] = torch.stack([b[k] for b in batch])\n",
        "    return out\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "num_epochs = 2\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=100,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels_concepts = batch[\"labels_concepts\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=input_ids\n",
        "        )\n",
        "\n",
        "        vision_outputs = model.vision_tower(pixel_values)\n",
        "        vision_feat = vision_outputs.last_hidden_state.mean(dim=1)\n",
        "        logits = classifier(vision_feat)\n",
        "\n",
        "        loss_cls = bce_loss(logits, labels_concepts)\n",
        "        loss = outputs.loss + loss_cls\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(f\"Epoch {epoch} | Step {step} | Loss {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch} Average Loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "1qDybjQnReyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ AUROC Evaluation"
      ],
      "metadata": {
        "id": "8-rC6Y0qm5H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "model.eval()\n",
        "classifier.eval()\n",
        "\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels_concepts = batch[\"labels_concepts\"].cpu().numpy()\n",
        "\n",
        "        vision_outputs = model.vision_tower(pixel_values)\n",
        "        vision_feat = vision_outputs.last_hidden_state.mean(dim=1)\n",
        "        logits = classifier(vision_feat)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "        all_labels.append(labels_concepts)\n",
        "        all_probs.append(probs)\n",
        "\n",
        "all_labels = np.vstack(all_labels)\n",
        "all_probs = np.vstack(all_probs)\n",
        "\n",
        "macro_auc = roc_auc_score(all_labels, all_probs, average=\"macro\")\n",
        "micro_auc = roc_auc_score(all_labels, all_probs, average=\"micro\")\n",
        "\n",
        "print(\"Macro AUROC:\", macro_auc)\n",
        "print(\"Micro AUROC:\", micro_auc)\n"
      ],
      "metadata": {
        "id": "3xCUY5AFRm6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Grad-CAM Implementation"
      ],
      "metadata": {
        "id": "E_Xwh8o7m-47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "def reshape_transform(tensor):\n",
        "    B, N, C = tensor.shape\n",
        "    H = W = int((N - 1) ** 0.5)\n",
        "    return tensor[:, 1:, :].permute(0, 2, 1).reshape(B, C, H, W)\n",
        "\n",
        "target_layer = model.vision_tower.vision_model.encoder.layers[-1]\n",
        "\n",
        "cam = GradCAM(\n",
        "    model=model.vision_tower,\n",
        "    target_layers=[target_layer],\n",
        "    reshape_transform=reshape_transform\n",
        ")\n"
      ],
      "metadata": {
        "id": "-XA7wDXlRzwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ Visualization Examples"
      ],
      "metadata": {
        "id": "7MB3uE1znCaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "example = test_ds[0]\n",
        "image = example[\"pixel_values\"].unsqueeze(0).to(device)\n",
        "\n",
        "grayscale_cam = cam(input_tensor=image)[0]\n",
        "\n",
        "orig_img = example[\"pixel_values\"].permute(1, 2, 0).cpu().numpy()\n",
        "orig_img = (orig_img - orig_img.min()) / (orig_img.max() - orig_img.min())\n",
        "\n",
        "cam_image = show_cam_on_image(orig_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cam_image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Grad-CAM (Vision Encoder)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "72ulCH8mR58E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ Inference Demo"
      ],
      "metadata": {
        "id": "SkdBJfvunG3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "example = test_ds[1]\n",
        "\n",
        "inputs = processor(\n",
        "    images=example[\"pixel_values\"],\n",
        "    text=build_prompt(),\n",
        "    return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128\n",
        "    )\n",
        "\n",
        "report = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Radiology Report:\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "YGycRD8JSBdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîü Notes on Limitations & Extensions"
      ],
      "metadata": {
        "id": "25DkhisXnKIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Weak supervision for concepts via keyword matching (can be replaced with expert labels)\n",
        "- ROCO captions are not full clinical reports\n",
        "- Larger batch sizes require stronger GPUs\n",
        "- Extend with:\n",
        "  ‚Ä¢ CheXpert / MIMIC-CXR labels\n",
        "  ‚Ä¢ LoRA fine-tuning\n",
        "  ‚Ä¢ Attention rollout explainability\n",
        "  ‚Ä¢ Clinical prompt engineering\n"
      ],
      "metadata": {
        "id": "KftXmvZAnOU7"
      }
    }
  ]
}